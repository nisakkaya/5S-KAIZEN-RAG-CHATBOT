{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 1Ô∏è‚É£ Gerekli k√ºt√ºphaneler\n",
        "# ============================================\n",
        "!pip install -q pdfplumber tqdm chromadb google-generativeai numpy langchain gradio\n",
        "\n",
        "import os\n",
        "import pdfplumber\n",
        "import numpy as np\n",
        "import chromadb\n",
        "import google.generativeai as genai\n",
        "import time\n",
        "import gradio as gr\n",
        "\n",
        "# ============================================\n",
        "# 2Ô∏è‚É£ API Key ayarƒ±\n",
        "# ============================================\n",
        "from google.colab import userdata\n",
        "API_KEY = userdata.get('API_KEY')\n",
        "genai.configure(api_key=API_KEY)\n",
        "\n",
        "EMBED_MODEL = \"models/text-embedding-004\"\n",
        "LLM_MODEL = \"models/gemini-2.5-flash\"\n",
        "\n",
        "# ============================================\n",
        "# 3Ô∏è‚É£ PDF'i oku ve chunk'lara b√∂l\n",
        "# ============================================\n",
        "# PDF oku\n",
        "# 1Ô∏è‚É£ Google Drive'ƒ± baƒüla\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2Ô∏è‚É£ PDF dosya yolunu ayarla\n",
        "pdf_path = \"/content/The Ultimate Guide to 5S and 5S Training _ KAIZEN‚Ñ¢ Article.pdf\"\n",
        "\n",
        "# 3Ô∏è‚É£ PDF'i oku ve chunk'lara b√∂l\n",
        "import pdfplumber\n",
        "\n",
        "with pdfplumber.open(pdf_path) as pdf:\n",
        "    text = \"\\n\".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
        "\n",
        "def chunk_text(text, size=500, overlap=50):\n",
        "    tokens = text.split()\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        chunks.append(\" \".join(tokens[i:i+size]))\n",
        "        i += size - overlap\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_text(text)\n",
        "print(f\"üìÑ PDF {len(chunks)} chunk'a b√∂l√ºnd√º.\")\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 4Ô∏è‚É£ Embedding olu≈ütur ve Chroma'ya kaydet\n",
        "# ============================================\n",
        "collection_name = \"kaizen_5s_rag\"\n",
        "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "collection = client.get_or_create_collection(name=collection_name)\n",
        "\n",
        "if collection.count() == 0:\n",
        "    print(\"‚è≥ Embeddingler olu≈üturuluyor ve Chroma'ya ekleniyor...\")\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        try:\n",
        "            result = genai.embed_content(model=EMBED_MODEL, content=chunk)\n",
        "            emb = np.array(result[\"embedding\"], dtype=\"float32\")\n",
        "            collection.add(\n",
        "                documents=[chunk],\n",
        "                embeddings=[emb.tolist()],\n",
        "                metadatas=[{\"chunk_id\": i}],\n",
        "                ids=[str(i)]\n",
        "            )\n",
        "            time.sleep(0.1)  # rate limit √∂nleme\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Chunk {i} embedding hatasƒ±:\", e)\n",
        "    print(\"‚úÖ Embeddingler ba≈üarƒ±yla olu≈üturuldu.\")\n",
        "else:\n",
        "    print(\"‚úÖ Chroma veritabanƒ± zaten dolu, y√ºkleniyor.\")\n",
        "\n",
        "# ============================================\n",
        "# 5Ô∏è‚É£ Retrieval fonksiyonu\n",
        "# ============================================\n",
        "def retrieve(query, top_k=2):\n",
        "    try:\n",
        "        query_emb = np.array(\n",
        "            genai.embed_content(model=EMBED_MODEL, content=query)[\"embedding\"],\n",
        "            dtype=\"float32\"\n",
        "        )\n",
        "        results = collection.query(\n",
        "            query_embeddings=[query_emb.tolist()],\n",
        "            n_results=top_k\n",
        "        )\n",
        "        docs = results.get(\"documents\", [[]])[0]\n",
        "        return docs\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è Retrieval hatasƒ±:\", e)\n",
        "        return []\n",
        "\n",
        "# ============================================\n",
        "# 6Ô∏è‚É£ LLM ile cevap √ºretimi\n",
        "# ============================================\n",
        "from google.generativeai import GenerativeModel\n",
        "model = genai.GenerativeModel(LLM_MODEL)\n",
        "\n",
        "def answer_query(query, top_k=2):\n",
        "    context_docs = retrieve(query, top_k)\n",
        "    if not context_docs:\n",
        "        return \"‚ö†Ô∏è ƒ∞lgili bilgi bulunamadƒ±.\"\n",
        "\n",
        "    context_text = \"\\n\\n\".join(context_docs)\n",
        "    prompt = f\"\"\"\n",
        "A≈üaƒüƒ±daki baƒülamƒ± kullanarak kullanƒ±cƒ± sorusuna T√ºrk√ße olarak net bir cevap ver.\n",
        "\n",
        "Baƒülam:\n",
        "{context_text}\n",
        "\n",
        "Soru:\n",
        "{query}\n",
        "\n",
        "Cevap:\n",
        "\"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        return getattr(response, \"text\", str(response)).strip()\n",
        "    except Exception as e:\n",
        "        return f\"‚ö†Ô∏è LLM hatasƒ±: {e}\"\n",
        "\n",
        "# ============================================\n",
        "# 7Ô∏è‚É£ Gradio aray√ºz√º\n",
        "# ============================================\n",
        "import gradio as gr\n",
        "\n",
        "# Chat fonksiyonu\n",
        "def chat_fn(message, history):\n",
        "    reply = answer_query(message)  # RAG + LLM fonksiyonu\n",
        "    if \"ilgili bilgi bulunamadƒ±\" in reply:\n",
        "        reply += \"\\n\\nüí° Genel Bilgi: 5S uygulamalarƒ±nda sƒ±k yapƒ±lan hatalar; standartlarƒ± g√ºncel tutmamak, g√∂rsel y√∂netimi ihmal etmek, √ßalƒ±≈üanlarƒ± eƒüitmemek, d√ºzeni s√ºrd√ºrememek gibi durumlar olabilir.\"\n",
        "    history.append((message, reply))\n",
        "    return history\n",
        "\n",
        "# Aray√ºz\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as chat_demo:\n",
        "    gr.Markdown(\"## üí¨ 5S & Kaizen √ñƒüretici Chatbot\")\n",
        "    gr.Markdown(\"Sorunu yaz ve g√∂nder!\")\n",
        "\n",
        "    chat_box = gr.Chatbot(label=\"Sohbet\")\n",
        "    user_input = gr.Textbox(placeholder=\"Sorunu yaz...\")\n",
        "    submit_btn = gr.Button(\"üöÄ G√∂nder\")\n",
        "\n",
        "    submit_btn.click(chat_fn, inputs=[user_input, chat_box], outputs=chat_box)\n",
        "\n",
        "chat_demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "id": "pvOIBLPoTDSu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "outputId": "b0373351-e928-4641-f32c-60e853e056ec"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ PDF 6 chunk'a b√∂l√ºnd√º.\n",
            "‚úÖ Chroma veritabanƒ± zaten dolu, y√ºkleniyor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2860022087.py:146: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chat_box = gr.Chatbot(label=\"Sohbet\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://cadfd4663b9f61b3fd.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://cadfd4663b9f61b3fd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://cadfd4663b9f61b3fd.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}