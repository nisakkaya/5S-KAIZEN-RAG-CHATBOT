# -*- coding: utf-8 -*-
"""5S CHATBOT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qRUrt-zvpnHWQ6SdQUL1GRtrkKhp_GAr
"""

# ============================================
# 1ï¸âƒ£ Gerekli kÃ¼tÃ¼phaneler
# ============================================

import os
import pdfplumber
import numpy as np
import chromadb
import google.generativeai as genai
import time
import gradio as gr

# ============================================
# 2ï¸âƒ£ API Key ayarÄ±
# ============================================
from google.colab import userdata
API_KEY = userdata.get('API_KEY')
genai.configure(api_key=API_KEY)

EMBED_MODEL = "models/text-embedding-004"
LLM_MODEL = "models/gemini-2.5-flash"


# 2ï¸âƒ£ PDF dosya yolunu ayarla ve PDF'i oku ve chunk'lara bÃ¶l
pdf_files = [
    'How 5S Can Improve Workplace Safety, Quality, and Processes - isixsigma.com.pdf',               
    'The Ultimate Guide to 5S and 5S Training _ KAIZENâ„¢ Article.pdf',
    'Toyota Production System _ Vision & Philosophy _ Company _ Toyota Motor Corporation Official Global Website.pdf_5s.pdf'
]

# PDF'den metin okuma ve chunklama
def load_and_chunk_pdf(pdf_path, size=500, overlap=50):
    with pdfplumber.open(pdf_path) as pdf:
        text = "\n".join([page.extract_text() for page in pdf.pages if page.extract_text()])
    tokens = text.split()
    chunks = []
    i = 0
    while i < len(tokens):
        chunks.append(" ".join(tokens[i:i+size]))
        i += size - overlap
    return chunks

texts = []
for path in pdf_files:
    if os.path.exists(path):
        texts.extend(load_and_chunk_pdf(path))
print(f"ğŸ“š {len(pdf_files)} PDF dosyasÄ±ndan toplam {len(texts)} chunk oluÅŸturuldu.")


# ============================================
# 4ï¸âƒ£ Embedding oluÅŸtur ve Chroma'ya kaydet
# ============================================
collection_name = "kaizen_5s_rag"
client = chromadb.PersistentClient(path="./chroma_db")
collection = client.get_or_create_collection(name=collection_name)

if collection.count() == 0:
    print("â³ Embeddingler oluÅŸturuluyor...")
    for i, chunk in enumerate(texts):
        try:
            emb = np.array(
                genai.embed_content(model=EMBED_MODEL, content=chunk)["embedding"],
                dtype="float32"
            )
            collection.add(
                documents=[chunk],
                embeddings=[emb.tolist()],
                metadatas=[{"chunk_id": i}],
                ids=[str(i)]
            )
            time.sleep(0.05)
        except Exception as e:
            print(f"âš ï¸ Chunk {i} embedding hatasÄ±: {e}")
    print("âœ… Embeddingler tamamlandÄ±.")
else:
    print("âœ… Var olan embeddingler yÃ¼klendi.")

# ============================================
# 5ï¸âƒ£ Retrieval fonksiyonu
# ============================================
def retrieve(query, top_k=8, min_score=None):
    query_emb = np.array(
        genai.embed_content(model=EMBED_MODEL, content=query)["embedding"],
        dtype="float32"
    )
    results = collection.query(query_embeddings=[query_emb.tolist()], n_results=top_k)
    docs = results.get("documents", [[]])[0]
    if min_score and "distances" in results:
        docs = [
            d for d, s in zip(docs, results["distances"][0])
            if s <= min_score
        ]
    return docs

# ============================================
# 6ï¸âƒ£ LLM ile cevap Ã¼retimi
# ============================================
model = genai.GenerativeModel(LLM_MODEL)

def answer_query(query):
    docs = retrieve(query, top_k=8)
    if not docs:
        return "âš ï¸ Ä°lgili bilgi bulunamadÄ±."
    context = "\n\n".join(docs)
    prompt = f"""
AÅŸaÄŸÄ±daki baÄŸlamÄ± kullanarak kullanÄ±cÄ± sorusuna TÃ¼rkÃ§e olarak aÃ§Ä±klayÄ±cÄ±, profesyonel bir cevap ver.
Ã–nemli noktalarÄ± madde madde belirt.

BaÄŸlam:
{context}

Soru:
{query}

Cevap:
"""
    try:
        response = model.generate_content(prompt)
        return getattr(response, "text", str(response)).strip()
    except Exception as e:
        return f"âš ï¸ LLM hatasÄ±: {e}"

# ============================================
# 7ï¸âƒ£ Gradio arayÃ¼zÃ¼
# ============================================
def chat_fn(message, history):
    reply = answer_query(message)
    if "Ä°lgili bilgi bulunamadÄ±" in reply:
        reply += "\n\nğŸ’¡ Genel Bilgi: 5S uygulamalarÄ±nda standartlaÅŸmayÄ± sÃ¼rdÃ¼rememek, gÃ¶rsel yÃ¶netimi ihmal etmek ve Ã§alÄ±ÅŸan katÄ±lÄ±mÄ±nÄ± az tutmak yaygÄ±n hatalardandÄ±r."
    history.append((message, reply))
    return history

with gr.Blocks(theme=gr.themes.Monochrome()) as demo:
    gr.Markdown(
        """
        # ğŸ§  5S & Kaizen Bilgi AsistanÄ±  
        **RAG TabanlÄ± Yapay Zeka Chatbot**
        > ğŸ“˜ Kaynaklar: Kaizen Institute,Toyota Production System, iSixSigma
        """
    )

# GÃ¶rsel ekleme
    gr.Image("/content/5s-kaizen-slide1.png", elem_id="5s-gorsel", interactive=False)

    with gr.Row():
        with gr.Column(scale=3):
            chat_box = gr.Chatbot(label="ğŸ’¬ Sohbet Penceresi", height=450)
            with gr.Row():
                user_input = gr.Textbox(
                    placeholder="Sorunu buraya yaz...",
                    show_label=False,
                    scale=4
                )
                submit_btn = gr.Button("ğŸš€ GÃ¶nder", scale=1)
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ” Ã–rnek Sorular:")
            gr.Examples(
                examples=[
                    ["5S nedir?"],
                    ["Kaizen felsefesi neyi amaÃ§lar?"],
                    ["5S adÄ±mlarÄ±nÄ± aÃ§Ä±klar mÄ±sÄ±n?"],
                    ["Bir Ã¼retim hattÄ±nda 5S nasÄ±l uygulanÄ±r?"],
                ],
                inputs=[user_input]
            )
            gr.Markdown("### ğŸ“Š Kaynaklar:")
            gr.Markdown("- Kaizen Institute\n- Toyota Production System\n- iSixSigma")

    submit_btn.click(chat_fn, inputs=[user_input, chat_box], outputs=chat_box)

demo.launch()
