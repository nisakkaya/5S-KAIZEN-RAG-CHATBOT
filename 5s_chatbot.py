# -*- coding: utf-8 -*-
"""5S CHATBOT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qRUrt-zvpnHWQ6SdQUL1GRtrkKhp_GAr
"""

# ============================================
# 1Ô∏è‚É£ Gerekli k√ºt√ºphaneler
# ============================================

import os
import pdfplumber
import numpy as np
import chromadb
import google.generativeai as genai
import time
import gradio as gr

# ============================================
# 2Ô∏è‚É£ API Key ayarƒ±
# ============================================
from google.colab import userdata
API_KEY = userdata.get('API_KEY')
genai.configure(api_key=API_KEY)

EMBED_MODEL = "models/text-embedding-004"
LLM_MODEL = "models/gemini-2.5-flash"

# ============================================
# 3Ô∏è‚É£ PDF'i oku ve chunk'lara b√∂l
# ============================================
# PDF oku
# 1Ô∏è‚É£ Google Drive'ƒ± baƒüla
from google.colab import drive
drive.mount('/content/drive')

# 2Ô∏è‚É£ PDF dosya yolunu ayarla
pdf_path = "/content/The Ultimate Guide to 5S and 5S Training _ KAIZEN‚Ñ¢ Article.pdf"

# 3Ô∏è‚É£ PDF'i oku ve chunk'lara b√∂l
import pdfplumber

with pdfplumber.open(pdf_path) as pdf:
    text = "\n".join([page.extract_text() for page in pdf.pages if page.extract_text()])

def chunk_text(text, size=500, overlap=50):
    tokens = text.split()
    chunks = []
    i = 0
    while i < len(tokens):
        chunks.append(" ".join(tokens[i:i+size]))
        i += size - overlap
    return chunks

chunks = chunk_text(text)
print(f"üìÑ PDF {len(chunks)} chunk'a b√∂l√ºnd√º.")


# ============================================
# 4Ô∏è‚É£ Embedding olu≈ütur ve Chroma'ya kaydet
# ============================================
collection_name = "kaizen_5s_rag"
client = chromadb.PersistentClient(path="./chroma_db")
collection = client.get_or_create_collection(name=collection_name)

if collection.count() == 0:
    print("‚è≥ Embeddingler olu≈üturuluyor ve Chroma'ya ekleniyor...")
    for i, chunk in enumerate(chunks):
        try:
            result = genai.embed_content(model=EMBED_MODEL, content=chunk)
            emb = np.array(result["embedding"], dtype="float32")
            collection.add(
                documents=[chunk],
                embeddings=[emb.tolist()],
                metadatas=[{"chunk_id": i}],
                ids=[str(i)]
            )
            time.sleep(0.1)  # rate limit √∂nleme
        except Exception as e:
            print(f"‚ö†Ô∏è Chunk {i} embedding hatasƒ±:", e)
    print("‚úÖ Embeddingler ba≈üarƒ±yla olu≈üturuldu.")
else:
    print("‚úÖ Chroma veritabanƒ± zaten dolu, y√ºkleniyor.")

# ============================================
# 5Ô∏è‚É£ Retrieval fonksiyonu
# ============================================
def retrieve(query, top_k=2):
    try:
        query_emb = np.array(
            genai.embed_content(model=EMBED_MODEL, content=query)["embedding"],
            dtype="float32"
        )
        results = collection.query(
            query_embeddings=[query_emb.tolist()],
            n_results=top_k
        )
        docs = results.get("documents", [[]])[0]
        return docs
    except Exception as e:
        print("‚ö†Ô∏è Retrieval hatasƒ±:", e)
        return []

# ============================================
# 6Ô∏è‚É£ LLM ile cevap √ºretimi
# ============================================
from google.generativeai import GenerativeModel
model = genai.GenerativeModel(LLM_MODEL)

def answer_query(query, top_k=2):
    context_docs = retrieve(query, top_k)
    if not context_docs:
        return "‚ö†Ô∏è ƒ∞lgili bilgi bulunamadƒ±."

    context_text = "\n\n".join(context_docs)
    prompt = f"""
A≈üaƒüƒ±daki baƒülamƒ± kullanarak kullanƒ±cƒ± sorusuna T√ºrk√ße olarak net bir cevap ver.

Baƒülam:
{context_text}

Soru:
{query}

Cevap:
"""
    try:
        response = model.generate_content(prompt)
        return getattr(response, "text", str(response)).strip()
    except Exception as e:
        return f"‚ö†Ô∏è LLM hatasƒ±: {e}"

# ============================================
# 7Ô∏è‚É£ Gradio aray√ºz√º
# ============================================
import gradio as gr

# Chat fonksiyonu
def chat_fn(message, history):
    reply = answer_query(message)  # RAG + LLM fonksiyonu
    if "ilgili bilgi bulunamadƒ±" in reply:
        reply += "\n\nüí° Genel Bilgi: 5S uygulamalarƒ±nda sƒ±k yapƒ±lan hatalar; standartlarƒ± g√ºncel tutmamak, g√∂rsel y√∂netimi ihmal etmek, √ßalƒ±≈üanlarƒ± eƒüitmemek, d√ºzeni s√ºrd√ºrememek gibi durumlar olabilir."
    history.append((message, reply))
    return history

# Aray√ºz
with gr.Blocks(theme=gr.themes.Soft()) as chat_demo:
    gr.Markdown("## üí¨ 5S & Kaizen √ñƒüretici Chatbot")
    gr.Markdown("Sorunu yaz ve g√∂nder!")

    chat_box = gr.Chatbot(label="Sohbet")
    user_input = gr.Textbox(placeholder="Sorunu yaz...")
    submit_btn = gr.Button("üöÄ G√∂nder")

    submit_btn.click(chat_fn, inputs=[user_input, chat_box], outputs=chat_box)

chat_demo.launch()
