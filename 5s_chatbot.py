# -*- coding: utf-8 -*-
"""5S CHATBOT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qRUrt-zvpnHWQ6SdQUL1GRtrkKhp_GAr
"""

# ============================================
# 1️⃣ Gerekli kütüphaneler
# ============================================

import os
import pdfplumber
import numpy as np
import chromadb
import google.generativeai as genai
import time
import gradio as gr

# ============================================
# 2️⃣ API Key ayarı
# ============================================
from google.colab import userdata
API_KEY = userdata.get('API_KEY')
genai.configure(api_key=API_KEY)

EMBED_MODEL = "models/text-embedding-004"
LLM_MODEL = "models/gemini-2.5-flash"


# 2️⃣ PDF dosya yolunu ayarla ve PDF'i oku ve chunk'lara böl
pdf_files = [
    'How 5S Can Improve Workplace Safety, Quality, and Processes - isixsigma.com.pdf',               
    'The Ultimate Guide to 5S and 5S Training _ KAIZEN™ Article.pdf',
    'Toyota Production System _ Vision & Philosophy _ Company _ Toyota Motor Corporation Official Global Website.pdf_5s.pdf'
]

# PDF'den metin okuma ve chunklama
def load_and_chunk_pdf(pdf_path, size=500, overlap=50):
    with pdfplumber.open(pdf_path) as pdf:
        text = "\n".join([page.extract_text() for page in pdf.pages if page.extract_text()])
    tokens = text.split()
    chunks = []
    i = 0
    while i < len(tokens):
        chunks.append(" ".join(tokens[i:i+size]))
        i += size - overlap
    return chunks

texts = []
for path in pdf_files:
    if os.path.exists(path):
        texts.extend(load_and_chunk_pdf(path))
print(f"📚 {len(pdf_files)} PDF dosyasından toplam {len(texts)} chunk oluşturuldu.")


# ============================================
# 4️⃣ Embedding oluştur ve Chroma'ya kaydet
# ============================================
collection_name = "kaizen_5s_rag"
client = chromadb.PersistentClient(path="./chroma_db")
collection = client.get_or_create_collection(name=collection_name)

if collection.count() == 0:
    print("⏳ Embeddingler oluşturuluyor...")
    for i, chunk in enumerate(texts):
        try:
            emb = np.array(
                genai.embed_content(model=EMBED_MODEL, content=chunk)["embedding"],
                dtype="float32"
            )
            collection.add(
                documents=[chunk],
                embeddings=[emb.tolist()],
                metadatas=[{"chunk_id": i}],
                ids=[str(i)]
            )
            time.sleep(0.05)
        except Exception as e:
            print(f"⚠️ Chunk {i} embedding hatası: {e}")
    print("✅ Embeddingler tamamlandı.")
else:
    print("✅ Var olan embeddingler yüklendi.")

# ============================================
# 5️⃣ Retrieval fonksiyonu
# ============================================
def retrieve(query, top_k=8, min_score=None):
    query_emb = np.array(
        genai.embed_content(model=EMBED_MODEL, content=query)["embedding"],
        dtype="float32"
    )
    results = collection.query(query_embeddings=[query_emb.tolist()], n_results=top_k)
    docs = results.get("documents", [[]])[0]
    if min_score and "distances" in results:
        docs = [
            d for d, s in zip(docs, results["distances"][0])
            if s <= min_score
        ]
    return docs

# ============================================
# 6️⃣ LLM ile cevap üretimi
# ============================================
model = genai.GenerativeModel(LLM_MODEL)

def answer_query(query):
    docs = retrieve(query, top_k=8)
    if not docs:
        return "⚠️ İlgili bilgi bulunamadı."
    context = "\n\n".join(docs)
    prompt = f"""
Aşağıdaki bağlamı kullanarak kullanıcı sorusuna Türkçe olarak açıklayıcı, profesyonel bir cevap ver.
Önemli noktaları madde madde belirt.

Bağlam:
{context}

Soru:
{query}

Cevap:
"""
    try:
        response = model.generate_content(prompt)
        return getattr(response, "text", str(response)).strip()
    except Exception as e:
        return f"⚠️ LLM hatası: {e}"

# ============================================
# 7️⃣ Gradio arayüzü
# ============================================
def chat_fn(message, history):
    reply = answer_query(message)
    if "İlgili bilgi bulunamadı" in reply:
        reply += "\n\n💡 Genel Bilgi: 5S uygulamalarında standartlaşmayı sürdürememek, görsel yönetimi ihmal etmek ve çalışan katılımını az tutmak yaygın hatalardandır."
    history.append((message, reply))
    return history

with gr.Blocks(theme=gr.themes.Monochrome()) as demo:
    gr.Markdown(
        """
        # 🧠 5S & Kaizen Bilgi Asistanı  
        **RAG Tabanlı Yapay Zeka Chatbot**
        > 📘 Kaynaklar: Kaizen Institute,Toyota Production System, iSixSigma
        """
    )

# Görsel ekleme
    gr.Image("/content/5s-kaizen-slide1.png", elem_id="5s-gorsel", interactive=False)

    with gr.Row():
        with gr.Column(scale=3):
            chat_box = gr.Chatbot(label="💬 Sohbet Penceresi", height=450)
            with gr.Row():
                user_input = gr.Textbox(
                    placeholder="Sorunu buraya yaz...",
                    show_label=False,
                    scale=4
                )
                submit_btn = gr.Button("🚀 Gönder", scale=1)
        with gr.Column(scale=1):
            gr.Markdown("### 🔍 Örnek Sorular:")
            gr.Examples(
                examples=[
                    ["5S nedir?"],
                    ["Kaizen felsefesi neyi amaçlar?"],
                    ["5S adımlarını açıklar mısın?"],
                    ["Bir üretim hattında 5S nasıl uygulanır?"],
                ],
                inputs=[user_input]
            )
            gr.Markdown("### 📊 Kaynaklar:")
            gr.Markdown("- Kaizen Institute\n- Toyota Production System\n- iSixSigma")

    submit_btn.click(chat_fn, inputs=[user_input, chat_box], outputs=chat_box)

demo.launch()
